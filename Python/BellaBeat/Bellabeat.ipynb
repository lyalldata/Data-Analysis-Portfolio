{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    " \n",
    " ### Data Cleaning\n",
    " In this phase the data will be uploaded, origanized, cleaned and ready for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    }
   ],
   "source": [
    "# Reading files \n",
    "\n",
    "# Daily Datasets\n",
    "daily_activity  = pd.read_csv(\"/Users/gurilyall/Downloads/Analysis Projects/4. bellabeat_raw_data/csv_files/dailyActivity_merged.csv\")\n",
    "daily_calories   = pd.read_csv(\"/Users/gurilyall/Downloads/Analysis Projects/4. bellabeat_raw_data/csv_files/dailyCalories_merged.csv\")\n",
    "daily_intensities  = pd.read_csv(\"/Users/gurilyall/Downloads/Analysis Projects/4. bellabeat_raw_data/csv_files/dailyIntensities_merged.csv\")\n",
    "daily_steps  = pd.read_csv(\"/Users/gurilyall/Downloads/Analysis Projects/4. bellabeat_raw_data/csv_files/dailySteps_merged.csv\")\n",
    "\n",
    "# Other datasets\n",
    "heartrate_seconds  = pd.read_csv(\"/Users/gurilyall/Downloads/Analysis Projects/4. bellabeat_raw_data/csv_files/heartrate_seconds_merged.csv\")\n",
    "sleep_log  = pd.read_csv(\"/Users/gurilyall/Downloads/Analysis Projects/4. bellabeat_raw_data/csv_files/sleepDay_merged.csv\")\n",
    "weight_log  = pd.read_csv(\"/Users/gurilyall/Downloads/Analysis Projects/4. bellabeat_raw_data/csv_files/weightLogInfo_merged.csv\")\n",
    "print(\"successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Activity\n",
      " Index(['Id', 'ActivityDate', 'TotalSteps', 'TotalDistance', 'TrackerDistance',\n",
      "       'LoggedActivitiesDistance', 'VeryActiveDistance',\n",
      "       'ModeratelyActiveDistance', 'LightActiveDistance',\n",
      "       'SedentaryActiveDistance', 'VeryActiveMinutes', 'FairlyActiveMinutes',\n",
      "       'LightlyActiveMinutes', 'SedentaryMinutes', 'Calories'],\n",
      "      dtype='object') \n",
      "\n",
      "Daily Calories\n",
      " Index(['Id', 'ActivityDay', 'Calories'], dtype='object') \n",
      "\n",
      "Daily Intensities\n",
      " Index(['Id', 'ActivityDay', 'SedentaryMinutes', 'LightlyActiveMinutes',\n",
      "       'FairlyActiveMinutes', 'VeryActiveMinutes', 'SedentaryActiveDistance',\n",
      "       'LightActiveDistance', 'ModeratelyActiveDistance',\n",
      "       'VeryActiveDistance'],\n",
      "      dtype='object') \n",
      "\n",
      "Daily Steps\n",
      " Index(['Id', 'ActivityDay', 'StepTotal'], dtype='object') \n",
      "\n",
      "Heart Rate\n",
      " Index(['Id', 'Time', 'Value'], dtype='object') \n",
      "\n",
      "Sleep Log\n",
      " Index(['Id', 'SleepDay', 'TotalSleepRecords', 'TotalMinutesAsleep',\n",
      "       'TotalTimeInBed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Checking column names of all the dataframes\n",
    "\n",
    "print(\"Daily Activity\\n\",daily_activity.columns,\n",
    "      \"\\n\\nDaily Calories\\n\",daily_calories.columns,\n",
    "      \"\\n\\nDaily Intensities\\n\",daily_intensities.columns,\n",
    "      \"\\n\\nDaily Steps\\n\",daily_steps.columns,\n",
    "      \"\\n\\nHeart Rate\\n\",heartrate_seconds.columns,\n",
    "      \"\\n\\nSleep Log\\n\",sleep_log.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets check the size, shape & datatypes of all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity Size: 14100 \n",
      "activity Shape: (940, 15) \n",
      "Data types:\n",
      " Id                            int64\n",
      "ActivityDate                 object\n",
      "TotalSteps                    int64\n",
      "TotalDistance               float64\n",
      "TrackerDistance             float64\n",
      "LoggedActivitiesDistance    float64\n",
      "VeryActiveDistance          float64\n",
      "ModeratelyActiveDistance    float64\n",
      "LightActiveDistance         float64\n",
      "SedentaryActiveDistance     float64\n",
      "VeryActiveMinutes             int64\n",
      "FairlyActiveMinutes           int64\n",
      "LightlyActiveMinutes          int64\n",
      "SedentaryMinutes              int64\n",
      "Calories                      int64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "Calories Size: 2820 \n",
      "calories Shape: (940, 3) \n",
      "Data types:\n",
      " Id              int64\n",
      "ActivityDay    object\n",
      "Calories        int64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "Data Intensities Size: 9400 \n",
      "data Shape: (940, 10) \n",
      "Data types:\n",
      " Id                            int64\n",
      "ActivityDay                  object\n",
      "SedentaryMinutes              int64\n",
      "LightlyActiveMinutes          int64\n",
      "FairlyActiveMinutes           int64\n",
      "VeryActiveMinutes             int64\n",
      "SedentaryActiveDistance     float64\n",
      "LightActiveDistance         float64\n",
      "ModeratelyActiveDistance    float64\n",
      "VeryActiveDistance          float64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "Steps Size: 2820 \n",
      "steps Shape: (940, 3) \n",
      "Data types:\n",
      " Id              int64\n",
      "ActivityDay    object\n",
      "StepTotal       int64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "Heart Rate Size: 7450974 \n",
      "Heart Rate Shape: (2483658, 3) \n",
      "Data types:\n",
      " Id        int64\n",
      "Time     object\n",
      "Value     int64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "Sleep Log Size: 2065 \n",
      "Sleep Log Shape: (413, 5) \n",
      "Data types:\n",
      " Id                     int64\n",
      "SleepDay              object\n",
      "TotalSleepRecords      int64\n",
      "TotalMinutesAsleep     int64\n",
      "TotalTimeInBed         int64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "Weight Log Size: 536 \n",
      "Weight Log Shape: (67, 8) \n",
      "Data types:\n",
      " Id                  int64\n",
      "Date               object\n",
      "WeightKg          float64\n",
      "WeightPounds      float64\n",
      "Fat               float64\n",
      "BMI               float64\n",
      "IsManualReport       bool\n",
      "LogId               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# checking data shape, size and data types\n",
    "print('Activity Size:', daily_activity.size, \n",
    "      \"\\nactivity Shape:\" , daily_activity.shape, \n",
    "      '\\nData types:\\n',daily_activity.dtypes )\n",
    "\n",
    "print(f'{\"-\"*50}\\nCalories Size:', daily_calories.size, \n",
    "      \"\\ncalories Shape:\" , daily_calories.shape, \n",
    "      '\\nData types:\\n',daily_calories.dtypes )\n",
    "\n",
    "print(f'{\"-\"*50}\\nData Intensities Size:', daily_intensities.size, \n",
    "      \"\\ndata Shape:\" , daily_intensities.shape, \n",
    "      '\\nData types:\\n',daily_intensities.dtypes )\n",
    "\n",
    "print(f'{\"-\"*50}\\nSteps Size:', daily_steps.size, \n",
    "      \"\\nsteps Shape:\" , daily_steps.shape, \n",
    "      '\\nData types:\\n',daily_steps.dtypes )\n",
    "\n",
    "print(f'{\"-\"*50}\\nHeart Rate Size:', heartrate_seconds.size,\n",
    "      \"\\nHeart Rate Shape:\", heartrate_seconds.shape,\n",
    "      '\\nData types:\\n',heartrate_seconds.dtypes )\n",
    "\n",
    "print(f'{\"-\"*50}\\nSleep Log Size:', sleep_log.size,\n",
    "      \"\\nSleep Log Shape:\", sleep_log.shape,\n",
    "      '\\nData types:\\n',sleep_log.dtypes )\n",
    "\n",
    "print(f'{\"-\"*50}\\nWeight Log Size:', weight_log.size,\n",
    "      \"\\nWeight Log Shape:\", weight_log.shape,\n",
    "      '\\nData types:\\n',weight_log.dtypes )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will count the total number of participants by their unique IDs and analyze the frequency of their participation. By checking the value counts, we can assess the level of engagement and determine whether participants were actively involved in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_activity Participants: 33 \n",
      "daily_calories Participants: 33 \n",
      "daily_intensities Participants: 33 \n",
      "daily_steps Participants: 33 \n",
      "Heart Rate Participants: 14 \n",
      "Sleep Log Participants: 24 \n",
      "Weight Log Participants: 8\n"
     ]
    }
   ],
   "source": [
    "# checking the number of participants in each dataset\n",
    "print(\"daily_activity Participants:\", daily_activity.Id.nunique(),\n",
    "      \"\\ndaily_calories Participants:\", daily_calories.Id.nunique(),\n",
    "      \"\\ndaily_intensities Participants:\", daily_intensities.Id.nunique(),\n",
    "      \"\\ndaily_steps Participants:\", daily_steps.Id.nunique(),\n",
    "      \"\\nHeart Rate Participants:\", heartrate_seconds.Id.nunique(),\n",
    "      \"\\nSleep Log Participants:\", sleep_log.Id.nunique(),\n",
    "      \"\\nWeight Log Participants:\", weight_log.Id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows, `Sleep Log`,`Heart Rate` and `weight_log` have `24`,`14`,`8`  different participants log-ins . All the other datasets have `33`  log-ins from 33 participants.\n",
    "\n",
    "In the next step we will check the amounts of logins per dataset to check the participation of each person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Login(s) for each participant by ID\n",
      "\n",
      "             Activity  Calories  Intensities  Steps  Heartrate  SleepLog  WeightLog\n",
      "Id                                                                                \n",
      "1503960366        31        31           31     31        NaN      25.0        2.0\n",
      "4319703577        31        31           31     31        NaN      26.0        2.0\n",
      "8583815059        31        31           31     31        NaN       NaN        NaN\n",
      "8378563200        31        31           31     31        NaN      32.0        NaN\n",
      "8053475328        31        31           31     31        NaN       3.0        NaN\n",
      "7086361926        31        31           31     31        NaN      24.0        NaN\n",
      "6962181067        31        31           31     31   266326.0      31.0       30.0\n",
      "5553957443        31        31           31     31   255174.0      31.0        NaN\n",
      "4702921684        31        31           31     31        NaN      28.0        NaN\n",
      "4558609924        31        31           31     31   192168.0       5.0        5.0\n",
      "1624580081        31        31           31     31        NaN       NaN        NaN\n",
      "4388161847        31        31           31     31   249748.0      24.0        NaN\n",
      "4445114986        31        31           31     31        NaN      28.0        NaN\n",
      "8877689391        31        31           31     31   228841.0       NaN       24.0\n",
      "1927972279        31        31           31     31        NaN       5.0        1.0\n",
      "2873212765        31        31           31     31        NaN       NaN        2.0\n",
      "2320127002        31        31           31     31        NaN       1.0        NaN\n",
      "4020332650        31        31           31     31   285461.0       8.0        NaN\n",
      "2026352035        31        31           31     31     2490.0      28.0        NaN\n",
      "1844505072        31        31           31     31        NaN       3.0        NaN\n",
      "2022484408        31        31           31     31   154104.0       NaN        NaN\n",
      "3977333714        30        30           30     30        NaN      28.0        NaN\n",
      "1644430081        30        30           30     30        NaN       4.0        NaN\n",
      "5577150313        30        30           30     30   248560.0      26.0        1.0\n",
      "8792009665        29        29           29     29   122841.0      15.0        NaN\n",
      "6290855005        29        29           29     29        NaN       NaN        NaN\n",
      "6117666160        28        28           28     28   158899.0      18.0        NaN\n",
      "6775888955        26        26           26     26    32771.0       3.0        NaN\n",
      "7007744171        26        26           26     26   133592.0       2.0        NaN\n",
      "3372868164        20        20           20     20        NaN       NaN        NaN\n",
      "8253242879        19        19           19     19        NaN       NaN        NaN\n",
      "2347167796        18        18           18     18   152683.0      15.0        NaN\n",
      "4057192912         4         4            4      4        NaN       NaN        NaN\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.width', 1000)  \n",
    "pd.set_option('display.max_columns', None)  \n",
    "# checking login(s) for each participant by ID\n",
    "\n",
    "comparison = pd.concat([\n",
    "    daily_activity['Id'].value_counts().rename('Activity'),\n",
    "    daily_calories['Id'].value_counts().rename('Calories'),\n",
    "    daily_intensities['Id'].value_counts().rename('Intensities'),\n",
    "    daily_steps['Id'].value_counts().rename('Steps'),\n",
    "    heartrate_seconds['Id'].value_counts().rename('Heartrate'),\n",
    "    sleep_log['Id'].value_counts().rename('SleepLog'),\n",
    "    weight_log['Id'].value_counts().rename('WeightLog')\n",
    "], axis=1)\n",
    "\n",
    "print(f\"{\" \"*20}Login(s) for each participant by ID\\n\\n\",comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the daily datasets the particpation is good  except 1 women with `4` entries. Fewer number of women participated in heartrate_seconds, Sleep_log, weight_log according to that there are alot of `Nan`s in these 3 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are examining all the datasets to identify any missing data. This helps us understand the completeness of the datasets and determine how to handle any gaps or inconsistencies in the data moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_activity Missing Values: 0 \n",
      "daily_calories Missing Values: 0 \n",
      "daily_intensities Missing Values: 0 \n",
      "daily_steps Missing Values: 0 \n",
      "Heart Rate Missing Values: 0 \n",
      "Sleep Log Missing Values: 0 \n",
      "Weight Log Missing Values: 65\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values in each dataset\n",
    "print(\"daily_activity Missing Values:\", daily_activity.isnull().sum().sum(),  \n",
    "      \"\\ndaily_calories Missing Values:\", daily_calories.isnull().sum().sum(),  \n",
    "      \"\\ndaily_intensities Missing Values:\", daily_intensities.isnull().sum().sum(),  \n",
    "      \"\\ndaily_steps Missing Values:\", daily_steps.isnull().sum().sum(),  \n",
    "      \"\\nHeart Rate Missing Values:\", heartrate_seconds.isnull().sum().sum(),  \n",
    "      \"\\nSleep Log Missing Values:\", sleep_log.isnull().sum().sum(),\n",
    "      \"\\nWeight Log Missing Values:\", weight_log.isnull().sum().sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 0\n",
       "Date               0\n",
       "WeightKg           0\n",
       "WeightPounds       0\n",
       "Fat               65\n",
       "BMI                0\n",
       "IsManualReport     0\n",
       "LogId              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_log.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight_log dataset contains `67` rows of data, with `65` of them having null values in the `Fat` column. This indicates that most of the data is missing, limiting the insights that can be drawn from the Fat column. Meanwhile, this issue has been forwarded to the Marketing Analysis Team and Bellabeatâ€™s IT Team for further investigation. We will not drop the data and will continue with the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Duplicated entries in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_activity Duplicates: 0 \n",
      "daily_calories Duplicates: 0 \n",
      "daily_intensities Duplicates: 0 \n",
      "daily_steps Duplicates: 0 \n",
      "Heart Rate Duplicates: 0 \n",
      "Sleep Log Duplicates: 3 \n",
      "Weight Log Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# checking for duplicates\n",
    "print(\"daily_activity Duplicates:\", daily_activity.duplicated().sum(),  \n",
    "      \"\\ndaily_calories Duplicates:\", daily_calories.duplicated().sum(),  \n",
    "      \"\\ndaily_intensities Duplicates:\", daily_intensities.duplicated().sum(),  \n",
    "      \"\\ndaily_steps Duplicates:\", daily_steps.duplicated().sum(),  \n",
    "      \"\\nHeart Rate Duplicates:\", heartrate_seconds.duplicated().sum(),  \n",
    "      \"\\nSleep Log Duplicates:\", sleep_log.duplicated().sum(),\n",
    "      \"\\nWeight Log Duplicates:\", weight_log.duplicated().sum())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the output there are `3` duplicated entries in the sleep log dataset. In the next step, dropping the duplicated values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep Log Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# dropping duplicated and null data entries\n",
    "sleep_log.drop_duplicates(inplace=True)\n",
    "print(\"Sleep Log Duplicates:\", sleep_log.duplicated().sum())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze\n",
    "\n",
    "### Data Transformation\n",
    "In this Phase, we will transform the data using various methods, such as changing data types, renaming columns, merging datasets, and creating new columns or aliases. These changes will be made in a way that preserves the original data, allowing it to be accessed or reviewed if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
